{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step Back Prompting RAG chain with Groq's Whisper API and Langchain LCEL\n",
    "\n",
    "## Introduction\n",
    "\n",
    "This tutorial demonstrates how to create a powerful question-answering system for audio and video content by combining Groq's Whisper API for transcription and Step Back Prompting RAG (Retrieval-Augmented Generation) for enhanced comprehension and response generation. Read the paper on Step Back Prompting [here](https://arxiv.org/abs/2310.06117).\n",
    "\n",
    "Groq's Whisper API provides state-of-the-art speech recognition capabilities, allowing us to accurately and quicklt transcribe audio and video files.\n",
    "\n",
    "Step Back Prompting is a RAG technique that improves the quality of responses by encouraging the language model to \"step back\" and consider the broader context before answering specific questions. This approach helps in generating more comprehensive and accurate responses, especially for complex queries.\n",
    "\n",
    "By integrating these technologies, we'll create a system that can:\n",
    "1. Transcribe audio/video content\n",
    "2. Index the transcriptions for efficient retrieval\n",
    "3. Use Step Back Prompting RAG to generate insightful responses to user queries about the content\n",
    "\n",
    "This notebook will guide you through the process of:\n",
    "\n",
    "1. Setting up the environment and dependencies.\n",
    "2. Downloading and transcribing audio/video files using Groq's Whisper API.\n",
    "3. Splitting and indexing the transcriptions into a vectorstore.\n",
    "4. Setting up the Step Back Prompting RAG pipeline using Langchain's LCEL (LangChain Expression Language).\n",
    "5. Chat with the media.\n",
    "\n",
    "By the end of this tutorial, you'll have a powerful tool for extracting insights from audio and video content, combining the strengths of accurate transcription and advanced language understanding.\n",
    "\n",
    "You can create a developer account for free at https://console.groq.com/ and generate a free API key to follow this tutorial!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setting up the environment and dependencies\n",
    "\n",
    "In this section, we install and import the necessary libraries required for our media rag task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Installation\n",
    "%pip install langchain -q\n",
    "%pip install langchain_community -q\n",
    "%pip install langchain_groq -q\n",
    "%pip install yt_dlp -q\n",
    "%pip install pydub -q\n",
    "%pip install librosa -q\n",
    "%pip install openai -q\n",
    "%pip install langchain_chroma -q\n",
    "%pip install langchain_huggingface -q\n",
    "%pip install sentence-transformers -q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up environment variables\n",
    "- You can create your free Groq api key [here](https://console.groq.com/keys)\n",
    "- SAVE_DIR is the name of the directory you want to save all the downloaded media urls. By default it saves all the media within a `media` subfolder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"GROQ_API_KEY\"] = \"gsk_...\"\n",
    "os.environ[\"SAVE_DIR\"] = \"./media/\" # Directory you want to save the media files to\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\" # To suppress huggingface warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Downloading and transcribing audio/video files using Groq's Whisper API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create media downloading functions\n",
    "\n",
    "Let's create some functions to help us download youtube videos and media files from the internet.\n",
    "\n",
    "The `load_yt` function takes in a youtube video url and downloads the mp3 of the video to the `SAVE_DIR` directory.\n",
    "\n",
    "The `load_meda` function takes in a remote media url and downloads it to the `SAVE_DIR` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yt_dlp\n",
    "from typing import Optional, Dict, List\n",
    "import requests\n",
    "import os\n",
    "from urllib.parse import urlparse\n",
    "import uuid\n",
    "import tqdm\n",
    "\n",
    "def load_yt(\n",
    "    url: str, \n",
    "    save_dir: Optional[str] = os.getenv(\"SAVE_DIR\"),\n",
    "    ydl_opts: Optional[Dict] = None\n",
    ") -> str:\n",
    "    \"\"\"Download a youtube video to `save_dir`\"\"\"\n",
    "    if not save_dir:\n",
    "        raise ValueError(\"save_dir is not set and SAVE_DIR environment variable is not defined\")\n",
    "\n",
    "    if not ydl_opts:\n",
    "        ydl_opts = {\n",
    "            \"format\": \"worstaudio/worst\", # Select low quality audio to speed up download\n",
    "            \"postprocessors\": [{\n",
    "                \"key\": \"FFmpegExtractAudio\",\n",
    "                \"preferredcodec\": \"mp3\",\n",
    "                \"preferredquality\": \"32\",\n",
    "            }],\n",
    "            \"outtmpl\": os.path.join(save_dir, \"%(title)s.%(ext)s\"),\n",
    "            \"restrictfilenames\": True,  # This option replaces spaces and other problematic characters\n",
    "        }\n",
    "\n",
    "    with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n",
    "        info = ydl.extract_info(url, download=True)\n",
    "        filename = ydl.prepare_filename(info)\n",
    "        filename = os.path.splitext(filename)[0] + '.mp3'\n",
    "    return filename\n",
    "\n",
    "def load_media(\n",
    "    url: str, \n",
    "    save_dir: Optional[str] = os.getenv(\"SAVE_DIR\")\n",
    ") -> str:\n",
    "    \"\"\"Download a audio/video file to `save_dir`\"\"\"\n",
    "\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()\n",
    "\n",
    "    parsed_url = urlparse(url)\n",
    "    filename = os.path.basename(parsed_url.path)\n",
    "\n",
    "    if not filename:\n",
    "        filename = f\"downloaded_file_{uuid.uuid4().hex[:8]}\"\n",
    "\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    file_path = os.path.join(save_dir, filename)\n",
    "\n",
    "    with open(file_path, 'wb') as file:\n",
    "        file.write(response.content)\n",
    "\n",
    "    return file_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download and Transcribe\n",
    "\n",
    "Now we can download and transcribe a list of urls and use the `load_yt` and `load_media` functions to download and then Groq's Whisper API to transcribe the media.\n",
    "\n",
    "The transcribed text is then stored in the `transcribed_documents` list as a `Document`, which is a Langchain primitive that will allow us to index these transcripts easily later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[youtube] Extracting URL: https://www.youtube.com/watch?v=nvJ74ZSpDQ4\n",
      "[youtube] nvJ74ZSpDQ4: Downloading webpage\n",
      "[youtube] nvJ74ZSpDQ4: Downloading ios player API JSON\n",
      "[youtube] nvJ74ZSpDQ4: Downloading m3u8 information\n",
      "[info] nvJ74ZSpDQ4: Downloading 1 format(s): 233\n",
      "[hlsnative] Downloading m3u8 manifest\n",
      "[hlsnative] Total fragments: 303\n",
      "[download] Destination: ./media/AMA_-_Function_Calling_in_GroqCloud.mp4\n",
      "[download] 100% of   10.70MiB in 00:00:24 at 452.72KiB/s                  \n",
      "[ExtractAudio] Destination: ./media/AMA_-_Function_Calling_in_GroqCloud.mp3\n",
      "Deleting original file ./media/AMA_-_Function_Calling_in_GroqCloud.mp4 (pass -k to keep)\n",
      "filepath: ./media/AMA_-_Function_Calling_in_GroqCloud.mp3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 1/3 [00:46<01:33, 46.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[youtube] Extracting URL: https://www.youtube.com/watch?v=0epti7O0Yis\n",
      "[youtube] 0epti7O0Yis: Downloading webpage\n",
      "[youtube] 0epti7O0Yis: Downloading ios player API JSON\n",
      "[youtube] 0epti7O0Yis: Downloading m3u8 information\n",
      "[info] 0epti7O0Yis: Downloading 1 format(s): 233\n",
      "[hlsnative] Downloading m3u8 manifest\n",
      "[hlsnative] Total fragments: 72\n",
      "[download] Destination: ./media/Untold_story_of_AI_s_fastest_chip.mp4\n",
      "[download] 100% of    2.49MiB in 00:00:14 at 179.68KiB/s                \n",
      "[ExtractAudio] Destination: ./media/Untold_story_of_AI_s_fastest_chip.mp3\n",
      "Deleting original file ./media/Untold_story_of_AI_s_fastest_chip.mp4 (pass -k to keep)\n",
      "filepath: ./media/Untold_story_of_AI_s_fastest_chip.mp3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 2/3 [01:08<00:32, 32.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filepath: ./media/interview_speech-analytics.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [01:12<00:00, 24.26s/it]\n"
     ]
    }
   ],
   "source": [
    "from groq import Groq\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "client = Groq(\n",
    "    api_key=os.getenv(\"GROQ_API_KEY\")\n",
    ")\n",
    "\n",
    "save_dir = os.getenv(\"SAVE_DIR\")\n",
    "urls = [\n",
    "    \"https://www.youtube.com/watch?v=nvJ74ZSpDQ4\",\n",
    "    \"https://www.youtube.com/watch?v=0epti7O0Yis\",\n",
    "    \"https://static.deepgram.com/examples/interview_speech-analytics.wav\"\n",
    "]\n",
    "transcribed_documents = []\n",
    "\n",
    "for url in tqdm.tqdm(urls):\n",
    "    if \"youtube.com\" in url or \"youtu.be\" in url:\n",
    "        load_func = load_yt\n",
    "    else:\n",
    "        load_func = load_media\n",
    "\n",
    "    try:\n",
    "        # Download file\n",
    "        filepath = load_func(url)\n",
    "        print(\"filepath:\", filepath)\n",
    "\n",
    "        #Transcribe file\n",
    "\n",
    "        with open(filepath, \"rb\") as file:\n",
    "            transcript = client.audio.transcriptions.create(\n",
    "                file=(filepath, file.read()),\n",
    "                model=\"whisper-large-v3\",\n",
    "                prompt=\"GROQ, Groq, LPU, Mixtral, Mistral, Llama, Meta\", # Optional: Add keywords that can help the model transcribe propernouns\n",
    "                response_format=\"text\",\n",
    "                temperature=0.0  # Optional\n",
    "            )\n",
    "\n",
    "        transcribed_documents.append(Document(\n",
    "            page_content=transcript,\n",
    "            metadata={\n",
    "                'source': url,\n",
    "                'filepath': filepath\n",
    "            }\n",
    "        ))\n",
    "    except Exception as e:\n",
    "        print(f\"Error downloading {url[:6]}.. : {str(e)}\")\n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Splitting and indexing the transcriptions into a vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documents indexed: 15\n",
      "Total number of documents in vectorstore: 15\n"
     ]
    }
   ],
   "source": [
    "from langchain_chroma import Chroma\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "embed_model = HuggingFaceEmbeddings()\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    separators=[\n",
    "        \"\\n\\n\", \n",
    "        \"\\n\", \n",
    "        \" \",\n",
    "        \"\",\n",
    "        # For multilingual/non-english text\n",
    "        # \"\\u200b\",  # Zero-width space\n",
    "        # \"\\uff0c\",  # Fullwidth comma\n",
    "        # \"\\u3001\",  # Ideographic comma\n",
    "        # \"\\uff0e\",  # Fullwidth full stop\n",
    "        # \"\\u3002\",  # Ideographic full stop\n",
    "    ],\n",
    "    chunk_size=2500,\n",
    "    chunk_overlap=0,\n",
    "    length_function=len,\n",
    "    is_separator_regex=False,\n",
    ")\n",
    "documents = text_splitter.split_documents(transcribed_documents)\n",
    "vectorstore = Chroma.from_documents(documents, embedding=embed_model, collection_name=\"groq_media_rag\")\n",
    "retriever = vectorstore.as_retriever(\n",
    "    search_kwargs={\n",
    "        'k': 2 # Limit to 2 documents per retrieval\n",
    "    }\n",
    ")\n",
    "print(f\"Documents indexed: {len(documents)}\")\n",
    "print(f\"Total number of documents in vectorstore: {len(vectorstore.get()['ids'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Setting up the Step Back Prompting RAG pipeline using Langchain's LCEL (LangChain Expression Language)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Step Back Prompting Chain\n",
    "\n",
    "Step Back Prompting enhances our RAG system by rephrasing specific questions into more general forms, leading to better context retrieval and more comprehensive answers. Here's how we implement it:\n",
    "\n",
    "1. Define a `StepBackQuestions` class for structured output.\n",
    "2. Initialize the `ChatGroq` model with \"llama3-8b-8192\".\n",
    "3. Create a system prompt for question rephrasing.\n",
    "4. Set up a `ChatPromptTemplate` combining the system prompt and user question.\n",
    "5. Construct the `STEP_BACK_CHAIN` using Langchain's LCEL to:\n",
    "   - Take the user's question\n",
    "   - Generate a more generic question\n",
    "   - Retrieve relevant documents\n",
    "   - Combine retrieved documents into a single string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"inbound from companies featured in the S&P 500. To do this, they're undergoing a production ramp-up. There's another interesting part of this story. The entirety of the chip is made in the U.S., which is very unique from its chip-making counterparts that rely heavily on Taiwan's TSMC foundry. There are some critical questions that come from Groq's value proposition. Is speed going to matter so much that the company can take meaningful market share? Does a shorter time to render answers make a difference to the users of large language models? Also, when companies order chips from Groq, they have to order a ton of them. What Groq can handle with 578 LPU chips, NVIDIA can handle with two of their H100 GPU chips. Does this make scaling unfeasible? While it's unclear whether Groq will be able to turn its moment of virality into the backbone for fast AI compute, this is a moment that was a long time in the making for Ross and crew. And if we want real innovation, the kind that drives down costs, solves problems, and unlocks new solutions, then we're going to need many more companies like Groq fighting for a spot at the big boys table.\\nthe door. The clock was ticking on a startup dedicated to speeding things up. At the time, generative AI accounted for less than 1% of their efforts. Then LLMs hit the mainstream and Groq created a solution to meet the moment. They captured attention with a jaw-dropping demo. You are breaking performance records. The speed is definitely a differentiator and people notice it. We've gone viral this week, so it was a really rapid hockey stick. We literally just show people the demo and they always shocked at how fast it is compared to what they get on graphics processors We built a language process Anyone that regularly uses large language models will notice that this is unlike anything on the market The millisecond you hit enter, this thing is giving you the entirety of its response. To further put this speed in perspective, Groq chips enable LLMs to write a full book in about 100 seconds. Now, that would require a huge context window that their current open source LLMs don't have, but that's some insane speed. Groq's site went from hardly any users to over 400,000 signups in no time, as the company set the new gold standard for low latency. So, how did they make it so fast? The speed is enabled through Groq's LPU design that sequentially processes language tasks that the LLMs execute. To put it more simply, the chip is designed to do exactly this task, not a lot of other ancillary tasks that increase complexity and cost. It also has memory on the chip, which is a rarity. The LPU is tailored towards inference. Think of the actual messaging with a chat interface, rather than training. That phase requires intense GPUs like the one from NVIDIA. You can pretty easily deduce why speed at this level would matter. The first one that comes to mind is low latency conversations with AI. This means no awkward pauses as the machine processes your answer. And Ross loves to demo this feature. Tell me something most people don't know. Um, here's something interesting. Did you know that octopuses have three hearts? Can you book a reservation at a Michelin star restaurant Of course I made a reservation for you and your guests at a Michelin star restaurant called Idom Who the chef The chef at Idom Ross has said that LPU chips will enable significantly lower compute costs Because we're so much faster per chip produced, we're able to get a better cost basis and energy basis. This is clearly to attract startups and developers. But Chamath said they're also getting a lot of\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Adapted from: https://github.com/langchain-ai/langchain/blob/master/cookbook/stepback-qa.ipynb\n",
    "from pydantic import BaseModel, Field\n",
    "from operator import itemgetter\n",
    "\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnableLambda, RunnablePassthrough\n",
    "\n",
    "class StepBackQuestions(BaseModel):\n",
    "    question: str = Field(\n",
    "        description=\"a question\"\n",
    "    )\n",
    "\n",
    "STEP_BACK_LLM = ChatGroq(model=\"llama3-8b-8192\").with_structured_output(StepBackQuestions, method='function_calling')\n",
    "\n",
    "STEP_BACK_SYSTEM_PROMPT = \"\"\"\\\n",
    "You are an expert analyser. Your task is to step back and paraphrase a question to a more generic step-back question, which is easier to answer. Here are a few examples:\n",
    "Question: \"What is the primary advantage of Groq's LPU architecture?\n",
    "Answer: \"what are the key features of Groq's LPU design?\n",
    "\n",
    "Question: \"How does Groq's LPU compare to traditional GPUs in AI workloads?\n",
    "Answer: \"how does Groq's LPU performance compare to other AI chips?\\\n",
    "\"\"\"\n",
    "\n",
    "STEP_BACK_PROMPT = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", STEP_BACK_SYSTEM_PROMPT),\n",
    "    (\"human\", \"{question}\")\n",
    "])\n",
    "\n",
    "STEP_BACK_CHAIN = (\n",
    "    {\n",
    "        \"question\": RunnablePassthrough()\n",
    "    }\n",
    "    |\n",
    "    STEP_BACK_PROMPT\n",
    "    |\n",
    "    STEP_BACK_LLM\n",
    "    |\n",
    "    itemgetter(\"question\")\n",
    "    |\n",
    "    retriever\n",
    "    | \n",
    "    RunnableLambda(lambda docs: \"\\n\".join(doc.page_content for doc in (docs)))\n",
    ")\n",
    "STEP_BACK_CHAIN.invoke(\"Why is Groq so fast?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the Final RAG Pipeline with Step Back Prompting Chain\n",
    "\n",
    "This section combines the Step Back Prompting chain with our main RAG pipeline to create a comprehensive question-answering system. The pipeline:\n",
    "\n",
    "1. Initializes a Groq LLM for the final response generation.\n",
    "2. Defines a system prompt that instructs the model to use both regular and step-back contexts.\n",
    "3. Creates a chat prompt template combining the system prompt and user input.\n",
    "4. Sets up a function to format retrieved documents.\n",
    "5. Constructs the final RAG chain using LCEL, which:\n",
    "   - Retrieves and formats regular context\n",
    "   - Obtains step-back context from the previous chain\n",
    "   - Passes the user's input\n",
    "   - Generates a response using the LLM\n",
    "   - Parses the output to a string\n",
    "\n",
    "This integrated pipeline leverages both direct and generalized contexts to produce more informed and comprehensive answers.\n",
    "\n",
    "The code below implements this final RAG pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.documents import Document\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "rag_llm = ChatGroq(model='llama3-70b-8192', temperature=0.2)\n",
    "\n",
    "RAG_SYSTEM_PROMPT = \"\"\"\\\n",
    "You are an assistant for question-answering tasks. \\\n",
    "Use the following pieces of retrieved context given within delimiters to answer the human's questions.\n",
    "```\n",
    "{context}\n",
    "{step_back_context}\n",
    "```\"\"\"\n",
    "\n",
    "RAG_PROMPT = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", RAG_SYSTEM_PROMPT),\n",
    "    (\"human\", \"{input}\")\n",
    "])\n",
    "\n",
    "def format_docs(docs: List[Document]):\n",
    "    \"\"\"Format the retrieved documents\"\"\"\n",
    "    return \"\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "rag_chain = (\n",
    "    {\n",
    "        \"context\": retriever | RunnableLambda(lambda docs: \"\\n\".join(doc.page_content for doc in (docs))), # Use retriever to retrieve docs from vectorstore -> format the documents into a string\n",
    "        \"step_back_context\": STEP_BACK_CHAIN, # Retrieve step back chain docs\n",
    "        \"input\": RunnablePassthrough() # Propogate the 'input' variable to the next step\n",
    "    }\n",
    "    | RAG_PROMPT # format prompt with 'context' and 'input' variables\n",
    "    | rag_llm # get response from LLM using the formatteed prompt\n",
    "    | StrOutputParser() # Parse through LLM response to get only the string response\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Chat with the media\n",
    "\n",
    "We can call the `.invoke()` method to run the whole step back rag chain pipeline and query our media files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Groq is fast due to its LPU (Language Processing Unit) design, which is specifically tailored for sequential language processing tasks. This design allows it to process language tasks quickly and efficiently. Additionally, the LPU has memory on the chip, which is a rarity, and is optimized for inference, making it well-suited for tasks like chat interfaces. This unique design enables Groq to achieve speeds that are significantly faster than traditional graphics processing units (GPUs).\n",
      "Function calling on Groq is powerful because it allows for a sequential process of calling various tools, using the answer, and then going back to prepare a final answer. This process is similar to a single core processor that can fetch in memory, reason over that memory, use another tool, and then prepare a final answer. Additionally, Groq has an \"insane amount of budget\" when it comes to tokens per second, which enables this sequential process to happen efficiently. This capability has the potential to unlock new solutions and drive down costs.\n",
      "KPIs stands for Key Performance Indicators. In the context of the speech analytics space, KPIs refer to various metrics that companies use to measure and track their performance in areas such as compliance, human interaction, empathy, upsell aptitudes, and closing aptitudes. There are hundreds of KPIs that companies can monitor, but it's overwhelming for contact center managers to keep track of all of them. Successful companies focus on a few problem areas at a time, train their call center agents on a maximum of three skills at a time, and then move on to the next skill once they've mastered the previous one.\n"
     ]
    }
   ],
   "source": [
    "print(rag_chain.invoke(\"Why is Groq so fast?\"))\n",
    "print(rag_chain.invoke(\"Why is function calling on Groq powerful?\"))\n",
    "print(rag_chain.invoke(\"What are KPIs?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This notebook has demonstrated how to build a powerful question-answering system that combines Groq's Whisper API for media transcription with Step Back Prompting RAG for enhanced comprehension and response generation.\n",
    "\n",
    "This approach significantly enhances the quality of responses by considering broader contexts before answering specific questions. It demonstrates the power of combining cutting-edge transcription technology with advanced language understanding techniques.\n",
    "\n",
    "By following this tutorial, you've created a versatile tool that can extract insights from audio and video content, opening up new possibilities for content analysis, research, and information retrieval.\n",
    "\n",
    "We hope this tutorial has been informative and inspires you to explore further applications of these powerful technologies!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
